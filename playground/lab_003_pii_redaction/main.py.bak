# import os
# import logging
# import shutil
# import json

# from pydantic import ValidationError

# from fastapi import FastAPI, WebSocket, WebSocketDisconnect, UploadFile, HTTPException, status
# from fastapi.staticfiles import StaticFiles
# from fastapi.responses import FileResponse
# from llms.cli import get_conversation_handle_fn
# from llms.core import TEMP_FOLDER, query_chat_processing_fn
# from llms import protected_llm, vulnerable_llm
# from websocket_primitives import WSMessage, WSToggleMessage, WSChatMessage
# import soteria_sdk


# contexts: dict[WebSocket, str] = {}
# protection_modes: dict[WebSocket, bool] = {}


# app = FastAPI()

# app.mount("/static", StaticFiles(directory="static"), name="static")

# @app.get("/")
# async def serve_index():
#     return FileResponse("static/index.html")


# @app.post("/upload-document/")
# def upload_document_endpoint(file: UploadFile):
#     """
#     Handles document uploads via HTTP POST. The uploaded JSON file is saved temporarily,
#     embedded into the vector database, and then deleted.
#     """
#     if not file.filename:
#         raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="No file uploaded.")

#     temp_file_path = os.path.join(TEMP_FOLDER, file.filename)

#     try:

#         with open(temp_file_path, "wb") as buffer:
#             shutil.copyfileobj(file.file, buffer)

#         # Use appropriate processing function based on protection mode
#         # Check if any WebSocket connection has vulnerable mode, default to protected
#         is_protected = True
#         if protection_modes:
#             # Use the mode from the most recent WebSocket connection
#             is_protected = list(protection_modes.values())[-1]

#         if is_protected:
#             embedding_result = protected_llm.process_and_embed_file_protected(temp_file_path)
#         else:
#             embedding_result = vulnerable_llm.process_and_embed_file(temp_file_path)

#         if not embedding_result["success"]:
#             detail_message = embedding_result.get("error", "Embedding failed")
#             if embedding_result.get('details'):
#                 detail_message += f" Details: {embedding_result['details']}"
#             print(f"Document embedding failed: {detail_message}")
#             raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_message)

#         print(f"Document '{file.filename}' uploaded and embedded successfully.")
#         return {"filename": file.filename, "message": "File uploaded and embedded successfully."}
#     except HTTPException:
#         raise #
#     except Exception as e:
#         print(f"An unexpected error occurred during file upload or embedding for '{file.filename}': {e}", exc_info=True)
#         raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Failed to process file '{file.filename}': {e}")
#     finally:
#         # Ensure the temporary file is removed after processing, regardless of success or failure.
#         if os.path.exists(temp_file_path):
#             try:
#                 os.remove(temp_file_path)
#                 print(f"Temporary file '{temp_file_path}' removed.")
#             except Exception as e:
#                 print(f"Error removing temporary file '{temp_file_path}': {e}")


# @app.websocket("/ws")
# async def websocket_endpoint(websocket: WebSocket):
#     """
#     Handles WebSocket connections, allowing clients to send chat messages
#     and receive intelligent responses based on the embedded knowledge base.
#     """
#     await websocket.accept()
#     # Initialize an empty conversational context for this new client connection
#     contexts[websocket] = "The conversation has just begun."
#     protection_modes[websocket] = True  # Default to protected

#     try:
#         while True:
#             raw_data = await websocket.receive_text()
#             print(f"[WS_DEBUG] Received raw_data: {raw_data}")

#             message_root = None
#             try:
#                 message_wrapper = WSMessage.model_validate_json(raw_data)
#                 message = message_wrapper.root
#             except ValidationError as e:
#                 print(f"[WS_DEBUG] ValidationError for raw_data: {raw_data} - {e}. Skipping LLM call due to invalid message format.")
#                 continue


#             # Now, check the type of message_root
#             if isinstance(message_root, WSToggleMessage):
#                 await handle_ws_toggle_message(websocket, message_root) # Pass the actual WSToggleMessage
#                 print(f"[WS_DEBUG] Processed toggle for {websocket.client.host}:{websocket.client.port}. Protection mode is now: {protection_modes.get(websocket)}. Executing continue, skipping run_llm.")
#                 continue

#             elif isinstance(message_root, WSChatMessage):
#                 user_input = message_root.message.strip()
#                 if not user_input:
#                     print(f"[WS_DEBUG] Received empty chat message for {websocket.client.host}:{websocket.client.port}. Skipping LLM call.")
#                     continue

#                 print(f"[WS_DEBUG] Calling run_llm for a WSChatMessage from {websocket.client.host}:{websocket.client.port}.")
#                 result = run_llm(websocket, user_input)
#                 await websocket.send_text(result)

#             elif isinstance(message_root, str): # Handle plain string messages if that's still desired
#                 user_input = message_root.strip()
#                 if not user_input:
#                     print(f"[WS_DEBUG] Received empty string message for {websocket.client.host}:{websocket.client.port}. Skipping LLM call.")
#                     continue

#                 print(f"[WS_DEBUG] Calling run_llm for a raw string message from {websocket.client.host}:{websocket.client.port}.")
#                 result = run_llm(websocket, user_input)
#                 await websocket.send_text(result)

#             else:
#                 # Fallback for unexpected but valid WSMessage types that aren't WSToggleMessage or WSChatMessage or str
#                 print(f"[WS_DEBUG] Received unhandled valid WSMessage root type: {type(message_root)} from {websocket.client.host}:{websocket.client.port}. Skipping.")
#                 continue

#     except WebSocketDisconnect:
#         print(f"[WS_DEBUG] WebSocket disconnected for {websocket.client.host}:{websocket.client.port}")
#         if websocket in contexts:
#             del contexts[websocket]
#         if websocket in protection_modes:
#             del protection_modes[websocket]

# async def handle_ws_toggle_message(websocket: WebSocket, message: WSToggleMessage):
#     protection_modes[websocket] = message.protected
#     mode = "protected" if protection_modes[websocket] else "vulnerable"
#     print(f"[WS_DEBUG] Inside handle_ws_toggle_message: Protection mode set to {protection_modes[websocket]} for {websocket.client.host}:{websocket.client.port}")
#     await websocket.send_text(f"Mode switched to: {mode}")



# def run_llm(websocket: WebSocket, user_input: str) -> str:
#     context = contexts[websocket]
#     result = ""

#     current_protection_mode = protection_modes.get(websocket, True)

#     print(f"[LLM_DEBUG] Calling run_llm for {websocket.client.host}:{websocket.client.port}")
#     print(f"[LLM_DEBUG]   User input: '{user_input}'")
#     print(f"[LLM_DEBUG]   Resolved protection_mode: {current_protection_mode}")

#     upload_result = upload_document_endpoint()

#     if upload_result["success"]:
#         print("✓ File uploaded and embedded successfully. Now starting conversation mode.")
#         print("You can now ask questions about the content of your JSON file!")
#         print("Type 'quit' or 'exit' to end the conversation.")
#         print("-" * 50)
#         handle_conversation = get_conversation_handle_fn(query_chat_processing_fn)
#         handle_conversation()
#     else:
#         print(f"✗ File uploading and embedding failed: {upload_result['error']}. Cannot proceed to conversation.")
#         if 'details' in upload_result and upload_result['details']:
#             print(f"Additional details: {upload_result['details']}")
#     print("--- Workflow finished ---")

# if __name__ == "__main__":
#     run_llm()
